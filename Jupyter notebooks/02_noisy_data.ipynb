{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Noisy data\n",
    "In this notebook, we will explore how data quality can affect predictions. We will simulate examples where noise is introduced in the test set, the training set, or both, and examine\n",
    "the effects on model performance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import modular.samples_setup as cs\n",
    "from modular import engine\n",
    "from modular import model_builder\n",
    "from modular import extra_functions as ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data simulation\n",
    "\n",
    "Let's start by simulating some data. We will simulate a data set containing 5000 samples each of\n",
    "squares and circles. We will save 20% of the data for test and we will use the rest for\n",
    "train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulate data \n",
    "SEED = 262\n",
    "n_samples = [5000]*2\n",
    "\n",
    "output = cs.generate_sample(n=n_samples, noise_prop=0, var=0, seed=SEED)\n",
    "images, labels = (output['images'], output['labels'])\n",
    "\n",
    "# Split test and train\n",
    "n_test = int(sum(n_samples)*0.2)\n",
    "test_index = np.arange(n_test)\n",
    "train_index = np.arange(n_test, sum(n_samples))\n",
    "\n",
    "images_test = images[test_index]\n",
    "images_train = images[train_index]\n",
    "\n",
    "label_test = labels[test_index]\n",
    "label_train = labels[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scenario with no noise\n",
    "Let's start by training the model in a scenario where both the training and the test sets are noise-free, ensuring all samples are perfectly clear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | test_ce: 5.07312 | test_acc: 75.6000\n",
      "Epoch: 2 | test_ce: 4.91997 | test_acc: 75.3500\n",
      "Epoch: 3 | test_ce: 4.66924 | test_acc: 78.8500\n",
      "Epoch: 4 | test_ce: 3.19666 | test_acc: 83.8000\n",
      "Epoch: 5 | test_ce: 0.96844 | test_acc: 95.2500\n",
      "Epoch: 6 | test_ce: 0.52768 | test_acc: 96.8500\n",
      "Epoch: 7 | test_ce: 0.48824 | test_acc: 97.4000\n",
      "Epoch: 8 | test_ce: 0.47484 | test_acc: 97.4000\n",
      "Epoch: 9 | test_ce: 0.45586 | test_acc: 97.4000\n",
      "Epoch: 10 | test_ce: 0.46344 | test_acc: 97.4000\n",
      "Epoch: 11 | test_ce: 0.44879 | test_acc: 97.3500\n",
      "Epoch: 12 | test_ce: 0.45594 | test_acc: 97.4000\n"
     ]
    }
   ],
   "source": [
    "# To reduce variability when re-running \n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 12\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create tensor\n",
    "X_test = torch.from_numpy(images_test).type(torch.float)\n",
    "X_train = torch.from_numpy(images_train).type(torch.float)\n",
    "\n",
    "y_train = torch.from_numpy(label_train).type(torch.long)\n",
    "y_test = torch.from_numpy(label_test).type(torch.long)\n",
    "\n",
    "## Add channel at dimension 1 (greyscale)\n",
    "X_train = X_train.unsqueeze(1)  \n",
    "X_test = X_test.unsqueeze(1)  \n",
    "        \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "        \n",
    "# Create data loader and turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    "    ) \n",
    "                           \n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model_1 =  model_builder.TVGG(\n",
    "    input_shape=1,  \n",
    "    hidden_units=10, \n",
    "    output_shape=2\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n",
    "\n",
    "output_1 = engine.train_test_loop(\n",
    "    model=model_1,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer, \n",
    "    loss_fn=loss_fn,\n",
    "    epochs=EPOCHS,\n",
    "    print_b=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good!\n",
    "\n",
    "#### 3. Scenario with noise in the test set\n",
    "What happens if out test set is entirely noisy, while the training set remains noise-free?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | test_ce: 6.02080 | test_acc: 67.5500\n",
      "Epoch: 2 | test_ce: 5.67422 | test_acc: 68.1000\n",
      "Epoch: 3 | test_ce: 5.91881 | test_acc: 66.0500\n",
      "Epoch: 4 | test_ce: 7.33350 | test_acc: 58.1000\n",
      "Epoch: 5 | test_ce: 8.68649 | test_acc: 50.2000\n",
      "Epoch: 6 | test_ce: 8.29231 | test_acc: 50.8000\n",
      "Epoch: 7 | test_ce: 7.98258 | test_acc: 51.5000\n",
      "Epoch: 8 | test_ce: 7.58947 | test_acc: 53.8500\n",
      "Epoch: 9 | test_ce: 7.94438 | test_acc: 51.6000\n",
      "Epoch: 10 | test_ce: 7.96077 | test_acc: 50.9500\n",
      "Epoch: 11 | test_ce: 7.94166 | test_acc: 51.3000\n",
      "Epoch: 12 | test_ce: 7.90330 | test_acc: 51.4500\n"
     ]
    }
   ],
   "source": [
    "# Adding noise only to the test set\n",
    "VAR = 0.15\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 12\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create copy of image sets\n",
    "\n",
    "images_test_e1 = images_test.copy()\n",
    "images_train_e1 = images_train.copy()\n",
    "\n",
    "label_test = labels[test_index]\n",
    "label_train = labels[train_index]\n",
    "\n",
    "# Add noise        \n",
    "for i in range(len(images_test)):\n",
    "        images_test_e1[i] = ef.add_gaussian_noise(\n",
    "                image=images_test_e1[i],\n",
    "                var=VAR\n",
    "        )\n",
    "\n",
    "# Create tensor\n",
    "X_test = torch.from_numpy(images_test_e1).type(torch.float)\n",
    "X_train = torch.from_numpy(images_train_e1).type(torch.float)\n",
    "\n",
    "y_test = torch.from_numpy(label_test).type(torch.long)\n",
    "y_train = torch.from_numpy(label_train).type(torch.long)\n",
    "\n",
    "## Add channel at dimension 1 (greyscale)\n",
    "X_train = X_train.unsqueeze(1)  \n",
    "X_test = X_test.unsqueeze(1)  \n",
    "        \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "        \n",
    "# Create data loader and turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "        ) \n",
    "                            \n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model_2 =  model_builder.TVGG(\n",
    "        input_shape=1,  \n",
    "        hidden_units=10, \n",
    "        output_shape=2\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)\n",
    "\n",
    "output_2 = engine.train_test_loop(\n",
    "        model=model_2,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader, \n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=EPOCHS,\n",
    "        print_b=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really worsens!\n",
    "\n",
    "#### 3. Scenario with noise in the train set\n",
    "What happens if our training set is entirely noisy while the test set remains noise-free?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | test_ce: 6.57604 | test_acc: 75.3000\n",
      "Epoch: 2 | test_ce: 5.96900 | test_acc: 75.3000\n",
      "Epoch: 3 | test_ce: 5.64955 | test_acc: 72.9000\n",
      "Epoch: 4 | test_ce: 5.27190 | test_acc: 73.0500\n",
      "Epoch: 5 | test_ce: 4.25713 | test_acc: 76.9000\n",
      "Epoch: 6 | test_ce: 2.47657 | test_acc: 85.2000\n",
      "Epoch: 7 | test_ce: 0.88503 | test_acc: 93.9000\n",
      "Epoch: 8 | test_ce: 0.59424 | test_acc: 96.1500\n",
      "Epoch: 9 | test_ce: 0.45314 | test_acc: 97.1500\n",
      "Epoch: 10 | test_ce: 0.46082 | test_acc: 97.4000\n",
      "Epoch: 11 | test_ce: 0.46469 | test_acc: 97.4500\n",
      "Epoch: 12 | test_ce: 0.43979 | test_acc: 97.4500\n"
     ]
    }
   ],
   "source": [
    "# Adding noise only to the train set\n",
    "VAR = 0.15\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 12\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create copy of image sets\n",
    "images_test_e2 = images_test.copy()\n",
    "images_train_e2 = images_train.copy()\n",
    "\n",
    "label_test = labels[test_index]\n",
    "label_train = labels[train_index]\n",
    "\n",
    "# add Gaussian Noise       \n",
    "for i in range(len(images_test)):\n",
    "        images_train_e2[i] = ef.add_gaussian_noise(\n",
    "                image=images_train_e2[i],\n",
    "                var=VAR\n",
    "                )\n",
    "\n",
    "# Create tensor\n",
    "X_test = torch.from_numpy(images_test_e2).type(torch.float)\n",
    "X_train = torch.from_numpy(images_train_e2).type(torch.float)\n",
    "\n",
    "y_test = torch.from_numpy(label_test).type(torch.long)\n",
    "y_train = torch.from_numpy(label_train).type(torch.long)\n",
    "\n",
    "## Add channel at dimension 1 (greyscale)\n",
    "X_train = X_train.unsqueeze(1)  \n",
    "X_test = X_test.unsqueeze(1)  \n",
    "        \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "        \n",
    "# Create data loader and turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "        ) \n",
    "                            \n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model_3 =  model_builder.TVGG(\n",
    "        input_shape=1,  \n",
    "        hidden_units=10, \n",
    "        output_shape=2\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1)\n",
    "\n",
    "output_3 = engine.train_test_loop(\n",
    "        model=model_3,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader, \n",
    "        optimizer=optimizer, \n",
    "        loss_fn=loss_fn,\n",
    "        epochs=EPOCHS,\n",
    "        print_b=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not so bad!\n",
    "\n",
    "#### 4. Scenario with noise in both sets\n",
    "Let's see an example where we have noisy data in both sets. We will add noise to half\n",
    "of the training set and half of the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | test_ce: 6.03790 | test_acc: 71.2000\n",
      "Epoch: 2 | test_ce: 5.70188 | test_acc: 71.4500\n",
      "Epoch: 3 | test_ce: 5.40449 | test_acc: 71.4000\n",
      "Epoch: 4 | test_ce: 5.23908 | test_acc: 71.8000\n",
      "Epoch: 5 | test_ce: 5.18184 | test_acc: 71.4000\n",
      "Epoch: 6 | test_ce: 3.70739 | test_acc: 78.2000\n",
      "Epoch: 7 | test_ce: 3.05427 | test_acc: 80.7500\n",
      "Epoch: 8 | test_ce: 2.75043 | test_acc: 81.8000\n",
      "Epoch: 9 | test_ce: 2.50962 | test_acc: 84.0500\n",
      "Epoch: 10 | test_ce: 2.38252 | test_acc: 85.1500\n",
      "Epoch: 11 | test_ce: 2.41362 | test_acc: 85.2000\n",
      "Epoch: 12 | test_ce: 2.39461 | test_acc: 85.5500\n"
     ]
    }
   ],
   "source": [
    "VAR =0.15\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 12\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create copy of image sets\n",
    "images_test_e3 = images_test.copy()\n",
    "images_train_e3 = images_train.copy()\n",
    "\n",
    "# add noise to half of them \n",
    "n_test_noisy = int(len(images_test)*0.5)\n",
    "n_train_noisy = int(len(images_train)*0.5)\n",
    "\n",
    "# add Gaussian Noise       \n",
    "for i in range(n_test_noisy):\n",
    "        images_test_e3[i] = ef.add_gaussian_noise(\n",
    "                image=images_test_e3[i],\n",
    "                var=VAR\n",
    "                )\n",
    "        \n",
    "for i in range(n_train_noisy):\n",
    "        images_train_e3[i] = ef.add_gaussian_noise(\n",
    "                image=images_train_e3[i],\n",
    "                var=VAR\n",
    "                )\n",
    "\n",
    "# Create tensor\n",
    "X_test = torch.from_numpy(images_test_e3).type(torch.float)\n",
    "X_train = torch.from_numpy(images_train_e3).type(torch.float)\n",
    "\n",
    "y_test = torch.from_numpy(label_test).type(torch.long)\n",
    "y_train = torch.from_numpy(label_train).type(torch.long)\n",
    "\n",
    "## Add channel at dimension 1 (greyscale)\n",
    "X_train = X_train.unsqueeze(1)  \n",
    "X_test = X_test.unsqueeze(1)  \n",
    "        \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "        \n",
    "# Create data loader and turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "        ) \n",
    "                        \n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model_4 =  model_builder.TVGG(\n",
    "        input_shape=1,  \n",
    "        hidden_units=10, \n",
    "        output_shape=2\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1)\n",
    "\n",
    "output_4 = engine.train_test_loop(\n",
    "        model=model_4,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader, \n",
    "        optimizer=optimizer, \n",
    "        loss_fn=loss_fn,\n",
    "        epochs=EPOCHS,\n",
    "        print_b=True\n",
    "        )                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
